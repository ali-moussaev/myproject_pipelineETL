Pipeline ETL - Analyse des Salaires Data Science ğŸ“Š
Description
Ce projet implÃ©mente un pipeline ETL (Extract, Transform, Load) pour l'analyse des salaires dans le domaine de la Data Science. Le pipeline collecte, nettoie, transforme et analyse les donnÃ©es salariales, en effectuant une conversion automatique des salaires USD/EUR, puis gÃ©nÃ¨re des rapports statistiques dÃ©taillÃ©s sur les salaires des professionnels de la Data Science.

Objectifs :
Extraire les donnÃ©es des fichiers CSV.
Transformer les donnÃ©es en nettoyant, normalisant et en convertissant les salaires dans la devise dÃ©sirÃ©e.
Charger les donnÃ©es dans une base de donnÃ©es PostgreSQL.
Analyser les donnÃ©es en fonction des diffÃ©rents critÃ¨res : salaires par poste, niveau d'expÃ©rience, mode de travail, etc.
ğŸŒŸ FonctionnalitÃ©s
Extraction des DonnÃ©es
Lecture optimisÃ©e de fichiers CSV avec validation automatique des donnÃ©es sources.
Gestion intelligente des types de donnÃ©es pour Ã©viter les erreurs lors de l'importation.
Transformation des DonnÃ©es
Nettoyage et normalisation des donnÃ©es pour garantir leur qualitÃ©.
Conversion automatique des salaires USD â†’ EUR via l'API ExchangeRate-API.
CatÃ©gorisation des postes et niveaux d'expÃ©rience pour mieux comprendre la rÃ©partition des salaires.
Gestion des valeurs manquantes et aberrantes pour assurer la cohÃ©rence des donnÃ©es.
Chargement des DonnÃ©es
Stockage des donnÃ©es dans une base de donnÃ©es PostgreSQL avec une gestion optimisÃ©e des connexions.
Validation des donnÃ©es avant insertion pour Ã©viter d'introduire des erreurs dans la base de donnÃ©es.
Analyses Statistiques
Statistiques par poste : analyse des salaires moyens par poste.
Analyses par niveau d'expÃ©rience : Ã©tude de l'impact de l'expÃ©rience sur les salaires.
Ã‰tude des modes de travail (tÃ©lÃ©travail/prÃ©sentiel/hybride) et leur influence sur les salaires.
GÃ©nÃ©ration de rapports dÃ©taillÃ©s avec logging pour assurer la traÃ§abilitÃ© des opÃ©rations.
ğŸ›  Technologies UtilisÃ©es
Python 3.8+ : Langage de programmation principal.
Pandas : Manipulation et analyse des donnÃ©es.
SQLAlchemy : ORM pour faciliter la gestion des interactions avec la base de donnÃ©es.
PostgreSQL : Base de donnÃ©es relationnelle pour le stockage des donnÃ©es.
Requests : UtilisÃ© pour effectuer des appels API pour la conversion de devise.
python-dotenv : Gestion des variables d'environnement sensibles.
logging : Gestion et traÃ§abilitÃ© des logs pour le suivi des actions.
ğŸ“‹ PrÃ©requis
Avant de commencer, assurez-vous que vous avez les Ã©lÃ©ments suivants :

Python 3.8 ou version supÃ©rieure.
Une base de donnÃ©es PostgreSQL installÃ©e et en cours d'exÃ©cution.
Une clÃ© API pour ExchangeRate-API (si vous souhaitez effectuer des conversions de devises).
ğŸš€ Installation
1. Cloner le repository
bash
Copier
Modifier
git clone https://github.com/votre-username/myproject_pipelineETL.git
cd myproject_pipelineETL
2. CrÃ©er un environnement virtuel
bash
Copier
Modifier
python -m venv .venv
source .venv/bin/activate  # Linux/MacOS
# ou
.venv\Scripts\activate     # Windows
3. Installer les dÃ©pendances
bash
Copier
Modifier
pip install -r requirements.txt
4. Configurer les variables d'environnement
Copiez le fichier .env.example vers .env :
bash
Copier
Modifier
cp .env.example .env
Remplissez les variables d'environnement dans le fichier .env :
env
Copier
Modifier
EXCHANGE_RATE_API_KEY=votre_clÃ©_api
DB_HOST=localhost
DB_PORT=5432
DB_USER=postgres
DB_PASSWORD=votre_mot_de_passe
DB_NAME=postgres
ğŸ“ Structure du Projet
Voici la structure du projet pour mieux comprendre son organisation :

bash
Copier
Modifier
myproject_pipelineETL/
â”‚
â”œâ”€â”€ data/               # Dossier pour les fichiers CSV
â”œâ”€â”€ logs/               # Dossier contenant les logs d'exÃ©cution
â”œâ”€â”€ src/                # Dossier contenant le code source
â”‚   â”œâ”€â”€ main.py         # Fichier principal pour exÃ©cuter le pipeline ETL
â”‚   â””â”€â”€ utils.py        # Fichier utilitaire pour les fonctions secondaires
â”œâ”€â”€ tests/              # Dossier contenant les tests unitaires
â”œâ”€â”€ .env                # Fichier des variables d'environnement
â”œâ”€â”€ requirements.txt    # Liste des dÃ©pendances Python
â””â”€â”€ README.md           # Ce fichier
ğŸ”§ Configuration de la Base de DonnÃ©es
CrÃ©er la base de donnÃ©es et la table
Avant de charger les donnÃ©es, vous devez configurer la base de donnÃ©es. ExÃ©cutez les commandes suivantes dans votre client PostgreSQL :

sql
Copier
Modifier
CREATE DATABASE data_science_salaries;
CREATE TABLE data_scientists_salaries (
    id SERIAL PRIMARY KEY,
    job_title VARCHAR(255),
    salary_in_usd NUMERIC,
    salary_in_eur NUMERIC,
    experience_level VARCHAR(50),
    work_setting VARCHAR(50)
);
Variables d'Environnement
EXCHANGE_RATE_API_KEY : ClÃ© API pour accÃ©der aux taux de change.
DB_HOST : HÃ´te de la base de donnÃ©es (par exemple localhost).
DB_PORT : Port de la base de donnÃ©es (par dÃ©faut 5432).
DB_USER : Utilisateur de la base de donnÃ©es.
DB_PASSWORD : Mot de passe de l'utilisateur.
DB_NAME : Nom de la base de donnÃ©es.
ğŸ“Š Utilisation
1. PrÃ©paration des donnÃ©es
Placez votre fichier CSV dans le dossier data/.
Assurez-vous que le fichier CSV contient les colonnes nÃ©cessaires :
job_title : IntitulÃ© du poste.
salary_in_usd : Salaire en USD.
experience_level : Niveau d'expÃ©rience (par exemple, Junior, Senior).
work_setting : Mode de travail (tÃ©lÃ©travail, hybride, prÃ©sentiel).
2. ExÃ©cution du pipeline ETL
Lancez le pipeline avec la commande suivante :

bash
Copier
Modifier
python src/main.py
3. Consultation des logs
Les logs sont disponibles dans le fichier logs/app.log. Les erreurs sont tracÃ©es avec les niveaux appropriÃ©s (INFO, WARNING, ERROR). Vous pouvez utiliser ces logs pour suivre l'exÃ©cution du pipeline.

ğŸ“ˆ Exemples d'Analyses
Voici quelques exemples de requÃªtes SQL pour analyser les donnÃ©es chargÃ©es dans PostgreSQL :

Salaires par Niveau d'ExpÃ©rience
sql
Copier
Modifier
SELECT 
    experience_level,
    ROUND(AVG(salary_in_eur)) AS avg_salary,
    COUNT(*) AS count
FROM data_scientists_salaries
GROUP BY experience_level
ORDER BY avg_salary DESC;
Distribution par Mode de Travail
sql
Copier
Modifier
SELECT 
    work_setting,
    COUNT(*) AS count,
    ROUND(AVG(salary_in_eur)) AS avg_salary
FROM data_scientists_salaries
GROUP BY work_setting;
ğŸ” Tests
Des tests unitaires sont inclus pour valider les diffÃ©rentes Ã©tapes du pipeline. Pour les exÃ©cuter, utilisez la commande suivante :

bash
Copier
Modifier
pytest tests/
ğŸ“ Logging
Le logging est configurÃ© pour assurer une traÃ§abilitÃ© des opÃ©rations. Le format des logs est le suivant :

pgsql
Copier
Modifier
timestamp - level - message
Les niveaux de log disponibles sont :

INFO : Pour les informations gÃ©nÃ©rales.
WARNING : Pour les avertissements.
ERROR : Pour les erreurs critiques.
ğŸ›¡ SÃ©curitÃ©
Les variables sensibles sont stockÃ©es dans le fichier .env, qui ne doit pas Ãªtre partagÃ©.
Toutes les entrÃ©es utilisateur et les donnÃ©es importÃ©es sont validÃ©es avant traitement.
Les connexions Ã  la base de donnÃ©es sont sÃ©curisÃ©es avec les bonnes pratiques de gestion des identifiants.
ğŸ¤ Contribution
Vous souhaitez contribuer au projet ? Voici comment faire :

Forkez le projet sur GitHub.
CrÃ©ez une branche pour votre fonctionnalitÃ© (git checkout -b feature/AmazingFeature).
Commitez vos changements (git commit -m 'Add AmazingFeature').
Poussez votre branche (git push origin feature/AmazingFeature).
Ouvrez une Pull Request pour discuter des modifications.
ğŸ“„ Licence
Ce projet est sous licence MIT.

ğŸ‘¥ Auteurs
Votre Nom - Travail initial - VotreGitHub
ğŸ™ Remerciements
ExchangeRate-API : Pour la fourniture des taux de change.
La communautÃ© Python : Pour les packages utilisÃ©s dans ce projet.