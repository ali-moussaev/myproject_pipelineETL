import pandas as pd
from sqlalchemy import create_engine, text
import requests
from dotenv import load_dotenv
import os
from pathlib import Path
import logging
from dataclasses import dataclass, field
from time import time
from typing import Optional, Dict, List
from retrying import retry

# =============================================================================
# CONFIGURATION DU LOGGING
# =============================================================================
# Configuration du syst√®me de logging pour tracer l'ex√©cution du script
# - Format: timestamp - nom du logger - niveau - message
# - Niveau: INFO pour capturer les informations importantes
# - Handlers: fichier (pour persistance) et console (pour debug en direct)
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO,
    handlers=[
        logging.FileHandler('app.log', encoding='utf-8'),  # Stockage permanent des logs
        logging.StreamHandler()  # Affichage en temps r√©el
    ]
)
logger = logging.getLogger(__name__)

# =============================================================================
# GESTION DES VARIABLES D'ENVIRONNEMENT
# =============================================================================
# V√©rification de l'existence du fichier .env contenant les cl√©s API
env_path = Path('api.env')
if not env_path.exists():
    logger.error(f"Le fichier .env n'existe pas dans le r√©pertoire courant. Chemin recherch√© : {env_path.absolute()}")
    exit()

# Chargement des variables d'environnement avec verbose=True pour le debug
load_dotenv(verbose=True)

# =============================================================================
# CONFIGURATION DU SERVICE DE TAUX DE CHANGE
# =============================================================================
# R√©cup√©ration de la cl√© API depuis les variables d'environnement
API_KEY = os.getenv('EXCHANGE_RATE_API_KEY')
logger.info(f"V√©rification de la cl√© API : {'Trouv√©e' if API_KEY else 'Non trouv√©e'}")

# Gestion du taux de change avec fallback sur une valeur par d√©faut
if not API_KEY:
    # Si pas de cl√© API, utilisation d'un taux fixe de r√©f√©rence
    logger.warning("La cl√© API n'a pas √©t√© trouv√©e dans le fichier .env")
    logger.info("Utilisation du taux de change de r√©f√©rence")
    TAUX_REFERENCE = 0.94  # Taux EUR/USD de r√©f√©rence
    USD_TO_EUR_RATE = TAUX_REFERENCE
    logger.info(f"Taux de conversion USD ‚Üí EUR de r√©f√©rence : {TAUX_REFERENCE}")
else:
    # Si cl√© API disponible, tentative de r√©cup√©ration du taux en temps r√©el
    BASE_URL = f"https://v6.exchangerate-api.com/v6/{API_KEY}/pair/USD/EUR"
    TAUX_REFERENCE = 0.94  # Taux de fallback en cas d'√©chec

    try:
        # Appel √† l'API de taux de change
        response = requests.get(BASE_URL)
        if response.status_code == 200:
            data = response.json()
            USD_TO_EUR_RATE = data['conversion_rate']
            logger.info(f"Taux de conversion USD ‚Üí EUR actuel : {USD_TO_EUR_RATE}")
        else:
            # En cas d'√©chec de l'API, utilisation du taux de r√©f√©rence
            logger.warning(f"L'API n'√©tant pas accessible, utilisation du taux de change de r√©f√©rence : {TAUX_REFERENCE}")
            USD_TO_EUR_RATE = TAUX_REFERENCE
    except Exception as e:
        logger.error(f"Erreur lors de la connexion √† l'API de taux de change: {e}")
        logger.warning(f"Utilisation du taux de change de r√©f√©rence : {TAUX_REFERENCE}")
        USD_TO_EUR_RATE = TAUX_REFERENCE

# =============================================================================
# CLASSES DE CONFIGURATION
# =============================================================================
@dataclass
class APIConfig:
    """
    Configuration pour l'API de taux de change
    
    Attributes:
        base_url (str): URL de base de l'API
        api_key (Optional[str]): Cl√© API pour l'authentification
        timeout (int): D√©lai maximum pour les requ√™tes en secondes
    """
    base_url: str
    api_key: Optional[str] = None
    timeout: int = 5

@dataclass
class DatabaseConfig:
    """
    Configuration pour la connexion √† la base de donn√©es PostgreSQL
    
    Attributes:
        host (str): H√¥te de la base de donn√©es
        port (int): Port de connexion
        user (str): Nom d'utilisateur
        password (str): Mot de passe
        database (str): Nom de la base de donn√©es
    """
    host: str
    port: int
    user: str
    password: str
    database: str

    @property
    def connection_string(self) -> str:
        """
        G√©n√®re la cha√Æne de connexion SQLAlchemy
        Returns:
            str: Cha√Æne de connexion format√©e
        """
        return f"postgresql+psycopg2://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}"

# =============================================================================
# SERVICES
# =============================================================================
class ExchangeRateService:
    """
    Service de gestion des taux de change avec syst√®me de cache
    
    Attributes:
        api_config (APIConfig): Configuration de l'API
        default_rate (float): Taux de change par d√©faut
        _last_update (float): Timestamp de la derni√®re mise √† jour
        _cache (Optional[float]): Valeur en cache du taux
    """
    def __init__(self, api_config: APIConfig):
        self.api_config = api_config
        self.default_rate = 0.94  # Taux de change par d√©faut
        self._last_update = 0  # Timestamp de la derni√®re mise √† jour
        self._cache = None  # Cache du taux de change

    @retry(stop_max_attempt_number=3, wait_exponential_multiplier=1000)
    def get_exchange_rate(self) -> float:
        """
        R√©cup√®re le taux de change avec gestion de cache
        - Met en cache le r√©sultat pendant 1 heure
        - R√©essaie 3 fois en cas d'√©chec avec d√©lai exponentiel
        
        Returns:
            float: Taux de change USD vers EUR
        """
        current_time = time()
        # V√©rification de la validit√© du cache (1 heure)
        if self._cache is not None and current_time - self._last_update < 3600:
            return self._cache

        # Si pas de cl√© API, retourne le taux par d√©faut
        if not self.api_config.api_key:
            logger.warning("Utilisation du taux de change par d√©faut")
            return self.default_rate

        try:
            # Tentative de r√©cup√©ration du nouveau taux
            response = requests.get(
                f"{self.api_config.base_url}/v6/{self.api_config.api_key}/pair/USD/EUR",
                timeout=self.api_config.timeout
            )
            if response.status_code == 200:
                rate = response.json()['conversion_rate']
                self._cache = rate
                self._last_update = current_time
                return rate
        except Exception as e:
            logger.error(f"Erreur API: {e}")
        
        return self.default_rate

# =============================================================================
# CLASSES DE TRAITEMENT DES DONN√âES
# =============================================================================
class DataValidator:
    """
    Classe de validation des donn√©es du DataFrame
    V√©rifie la structure et la qualit√© des donn√©es
    """
    @staticmethod
    def validate_dataframe(df: pd.DataFrame) -> bool:
        """
        Valide la structure et les donn√©es du DataFrame
        
        Args:
            df (pd.DataFrame): DataFrame √† valider
            
        Returns:
            bool: True si valide, False sinon
            
        Checks:
            - Pr√©sence des colonnes requises
            - Absence de valeurs nulles
            - Salaires positifs
        """
        required_columns = ['job_title', 'salary_in_usd', 'experience_level']
        
        try:
            # V√©rification des colonnes requises
            assert all(col in df.columns for col in required_columns)
            # V√©rification des valeurs nulles
            assert not df[required_columns].isnull().any().any()
            # V√©rification des salaires positifs
            assert df['salary_in_usd'].gt(0).all()
            return True
        except AssertionError:
            logger.error("Validation du DataFrame √©chou√©e")
            return False

class DataCleaner:
    """
    Classe de nettoyage et normalisation des donn√©es
    Applique des transformations pour uniformiser les donn√©es
    """
    @staticmethod
    def clean_text(text: str) -> str:
        """
        Nettoie et normalise une cha√Æne de texte
        
        Args:
            text (str): Texte √† nettoyer
            
        Returns:
            str: Texte nettoy√© et normalis√©
            
        Transformations:
            - Conversion en minuscules
            - Suppression des espaces en d√©but/fin
            - Gestion des valeurs nulles
        """
        if pd.isna(text):
            return text
        return str(text).strip().lower()

    def clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Nettoie l'ensemble du DataFrame
        
        Args:
            df (pd.DataFrame): DataFrame √† nettoyer
            
        Returns:
            pd.DataFrame: DataFrame nettoy√©
            
        Transformations:
            - Copie du DataFrame pour √©viter les modifications en place
            - Nettoyage des colonnes textuelles
        """
        df = df.copy()
        
        # Liste des colonnes textuelles √† nettoyer
        text_columns = ['job_title', 'job_category', 'experience_level', 'work_setting']
        for col in text_columns:
            if col in df.columns:
                df[col] = df[col].apply(self.clean_text)
                
        return df

# =============================================================================
# PIPELINE ETL PRINCIPAL
# =============================================================================

# === 1. EXTRACTION ===
csv_file_path = "C:/Users/Shonan/Desktop/MyDataEngineerProject/myproject_pipelineETL/jobs_in_data.csv"
logger.info("Extraction : Chargement du fichier CSV...")
try:
    # Lecture optimis√©e du CSV avec types sp√©cifi√©s
    df = pd.read_csv(
        csv_file_path,
        usecols=['job_title', 'salary_in_usd', 'experience_level', 'work_setting'],
        dtype={
            'job_title': 'category',  # Optimisation m√©moire pour les cha√Ænes r√©p√©t√©es
            'experience_level': 'category',
            'work_setting': 'category'
        }
    )
    # Logs des informations sur les donn√©es charg√©es
    logger.info(f"‚úì Fichier CSV charg√© avec succ√®s!")
    logger.info(f"‚Üí Nombre de lignes : {len(df)}")
    logger.info(f"‚Üí Colonnes pr√©sentes : {', '.join(df.columns)}")
    
    # Analyse d√©taill√©e de la structure des donn√©es
    logger.info("\n=== Analyse de la structure des donn√©es ===")
    logger.info("\n‚Üí Types de donn√©es pour chaque colonne :")
    logger.info(df.dtypes)
    
    logger.info("\n‚Üí Statistiques descriptives des colonnes num√©riques :")
    logger.info(df.describe())
    
    logger.info("\n‚Üí V√©rification des valeurs manquantes :")
    missing_values = df.isnull().sum()
    logger.info(missing_values[missing_values > 0])
    
    logger.info("\n‚Üí Exemple des premi√®res lignes :")
    logger.info(df.head(3))
    
    # V√©rification des valeurs uniques dans les colonnes cat√©gorielles
    categorical_columns = df.select_dtypes(include=['object']).columns
    logger.info("\n‚Üí Valeurs uniques dans les colonnes cat√©gorielles :")
    for col in categorical_columns:
        n_unique = df[col].nunique()
        logger.info(f"{col}: {n_unique} valeurs uniques")
        if n_unique < 10:  # Afficher les valeurs uniques seulement si leur nombre est raisonnable
            logger.info(f"Valeurs : {sorted(df[col].unique())}")

except FileNotFoundError:
    logger.error(f"‚ùå Erreur : Le fichier {csv_file_path} n'a pas √©t√© trouv√©.")
    exit()
except Exception as e:
    logger.error(f"‚ùå Erreur lors de la lecture du fichier : {str(e)}")
    exit()

# === 2. Nettoyage des donn√©es ===
logger.info("\n=== Nettoyage et uniformisation des donn√©es ===")

# Uniformiser les noms de colonnes en minuscules
df.columns = df.columns.str.lower()
logger.info("‚úì Noms des colonnes uniformis√©s en minuscules")

# Nettoyage des colonnes textuelles
text_columns = ['job_title', 'job_category', 'experience_level', 'employment_type', 'company_location', 'company_size']
for col in text_columns:
    if col in df.columns:
        df[col] = df[col].apply(DataCleaner.clean_text)
        logger.info(f"‚úì Colonne {col} : texte uniformis√© en minuscules et nettoy√©")

# Conversion et nettoyage des colonnes num√©riques
numeric_columns = ['salary', 'salary_in_usd']
for col in numeric_columns:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')
        # Suppression des valeurs aberrantes (optionnel)
        q1 = df[col].quantile(0.05)
        q3 = df[col].quantile(0.95)
        iqr = q3 - q1
        df = df[df[col].between(q1 - 1.5*iqr, q3 + 1.5*iqr)]
        logger.info(f"‚úì Colonne {col} : convertie en num√©rique et nettoy√©e des valeurs aberrantes")

# Conversion en EUR avec le taux actuel
if 'salary_in_usd' in df.columns:
    df['salary_in_eur'] = df['salary_in_usd'] * USD_TO_EUR_RATE
    logger.info("‚úì Conversion des salaires en EUR effectu√©e")

# Gestion des dates si pr√©sentes
date_columns = ['work_year']  # Ajoutez d'autres colonnes de dates si n√©cessaire
for col in date_columns:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')
        logger.info(f"‚úì Colonne {col} : convertie en ann√©e")

# V√©rification des valeurs manquantes apr√®s nettoyage
missing_values = df.isnull().sum()
if missing_values.any():
    logger.info("\n‚Üí Valeurs manquantes apr√®s nettoyage :")
    logger.info(missing_values[missing_values > 0])
    # Suppression des lignes avec des valeurs manquantes dans les colonnes critiques
    critical_columns = ['job_title', 'salary_in_usd', 'salary_in_eur']
    df = df.dropna(subset=critical_columns)
    logger.info("‚úì Suppression des lignes avec des valeurs manquantes dans les colonnes critiques")

# Affichage des statistiques apr√®s nettoyage
logger.info("\n=== Statistiques apr√®s nettoyage ===")
logger.info(f"Nombre de lignes final : {len(df)}")
logger.info("\nAper√ßu des donn√©es nettoy√©es :")
logger.info(df.head(3))

# === 3. Chargement dans PostgreSQL via SQLAlchemy ===
logger.info("Chargement des donn√©es dans PostgreSQL...")
# Remplace 'user' et 'password' par tes identifiants, et 'mydatas' par le nom de ta base
engine = create_engine("postgresql+psycopg2://postgres:admin@localhost:5432/postgres")
table_name = "data_scientists_salaries"  # Tu peux choisir un autre nom de table si n√©cessaire

try:
    # if_exists='replace' recr√©e la table √† chaque ex√©cution (ou utilise 'append' pour ajouter)
    df.to_sql(table_name, engine, if_exists='replace', index=False)
    logger.info(f"Donn√©es charg√©es avec succ√®s dans la table '{table_name}'.")
except Exception as e:
    logger.error("Erreur lors du chargement des donn√©es :", e)

# === 4. Analyse des salaires avec des requ√™tes SQL ===
logger.info("\n=== Analyses des salaires ===")

# V√©rification des colonnes disponibles
logger.info("\nColonnes disponibles dans la table :")
query_columns = f"""
SELECT column_name 
FROM information_schema.columns 
WHERE table_name = '{table_name}';
"""
with engine.connect() as conn:
    columns = conn.execute(text(query_columns))
    available_columns = [col[0] for col in columns]
    logger.info("Colonnes disponibles:", available_columns)

# 1. Analyse par intitul√© de poste
logger.info("\n1. Salaires moyens par intitul√© de poste :")
query_by_title = f"""
SELECT 
    job_title,
    CAST(AVG(salary_in_eur) AS DECIMAL(10,2)) AS avg_salary_eur,
    COUNT(*) AS total
FROM 
    {table_name}
GROUP BY 
    job_title
ORDER BY 
    avg_salary_eur DESC
LIMIT 10;
"""

# 2. Analyse d√©taill√©e par niveau d'exp√©rience
logger.info("\n2. Analyse des salaires par niveau d'exp√©rience :")
query_by_experience = f"""
SELECT 
    experience_level,
    CAST(AVG(salary_in_eur) AS DECIMAL(10,2)) AS salaire_moyen,
    CAST(MIN(salary_in_eur) AS DECIMAL(10,2)) AS salaire_min,
    CAST(MAX(salary_in_eur) AS DECIMAL(10,2)) AS salaire_max,
    CAST(STDDEV(salary_in_eur) AS DECIMAL(10,2)) AS ecart_type,
    COUNT(*) AS nombre_postes
FROM 
    {table_name}
GROUP BY 
    experience_level
ORDER BY 
    CASE 
        WHEN experience_level = 'entry' THEN 1
        WHEN experience_level = 'mid' THEN 2
        WHEN experience_level = 'senior' THEN 3
        WHEN experience_level = 'executive' THEN 4
        ELSE 5
    END;
"""

# 3. Analyse des salaires selon le mode de travail (remote/pr√©sentiel)
logger.info("\n3. Analyse des salaires selon le mode de travail :")
query_by_remote = f"""
WITH stats_by_remote AS (
    SELECT 
        CASE 
            WHEN work_setting = 'Remote' THEN 'T√©l√©travail'
            WHEN work_setting = 'In-person' THEN 'Pr√©sentiel'
            WHEN work_setting = 'Hybrid' THEN 'Hybride'
        END AS mode_travail,
        CAST(AVG(salary_in_eur) AS DECIMAL(10,2)) AS salaire_moyen,
        CAST(MIN(salary_in_eur) AS DECIMAL(10,2)) AS salaire_min,
        CAST(MAX(salary_in_eur) AS DECIMAL(10,2)) AS salaire_max,
        CAST(STDDEV(salary_in_eur) AS DECIMAL(10,2)) AS ecart_type,
        COUNT(*) AS nombre_postes,
        CAST(AVG(CASE WHEN experience_level = 'senior' THEN salary_in_eur END) AS DECIMAL(10,2)) AS moy_senior,
        CAST(AVG(CASE WHEN experience_level = 'mid' THEN salary_in_eur END) AS DECIMAL(10,2)) AS moy_mid,
        CAST(AVG(CASE WHEN experience_level = 'entry' THEN salary_in_eur END) AS DECIMAL(10,2)) AS moy_entry
    FROM 
        {table_name}
    GROUP BY 
        work_setting
)
SELECT 
    *,
    CAST(100.0 * nombre_postes / SUM(nombre_postes) OVER () AS DECIMAL(5,2)) as pourcentage_postes
FROM 
    stats_by_remote
ORDER BY 
    salaire_moyen DESC;
"""

# Ex√©cution des requ√™tes
with engine.connect() as conn:
    # Analyse par poste
    logger.info("\n=== Top 10 des postes les mieux r√©mun√©r√©s ===")
    result = conn.execute(text(query_by_title))
    for row in result:
        logger.info(f"üìä {row[0]}: {format(int(row[1]), ' ')}‚Ç¨ (Nombre de postes: {row[2]})")
    
    # Analyse par niveau d'exp√©rience
    logger.info("\n=== Analyse d√©taill√©e par niveau d'exp√©rience ===")
    result = conn.execute(text(query_by_experience))
    for row in result:
        logger.info(f"\nüìä Niveau : {row[0].upper()}")
        logger.info(f"   ‚Üí Salaire moyen : {format(int(row[1]), ' ')}‚Ç¨")
        logger.info(f"   ‚Üí Fourchette : {format(int(row[2]), ' ')}‚Ç¨ - {format(int(row[3]), ' ')}‚Ç¨")
        logger.info(f"   ‚Üí √âcart-type : {format(int(row[4]), ' ')}‚Ç¨")
        logger.info(f"   ‚Üí Nombre de postes : {row[5]}")

    # Analyse par mode de travail
    logger.info("\n=== Analyse d√©taill√©e par mode de travail ===")
    result = conn.execute(text(query_by_remote))
    for row in result:
        logger.info(f"\nüìä Mode : {row[0]}")
        logger.info(f"   ‚Üí Salaire moyen global : {format(int(row[1]), ' ')}‚Ç¨")
        logger.info(f"   ‚Üí Fourchette : {format(int(row[2]), ' ')}‚Ç¨ - {format(int(row[3]), ' ')}‚Ç¨")
        logger.info(f"   ‚Üí √âcart-type : {format(int(row[4]), ' ')}‚Ç¨")
        logger.info(f"   ‚Üí Nombre de postes : {row[5]} ({row[9]}% du total)")
        logger.info(f"   ‚Üí Moyenne par niveau :")
        if row[6]: logger.info(f"      ‚Ä¢ Senior : {format(int(row[6]), ' ')}‚Ç¨")
        if row[7]: logger.info(f"      ‚Ä¢ Mid : {format(int(row[7]), ' ')}‚Ç¨")
        if row[8]: logger.info(f"      ‚Ä¢ Entry : {format(int(row[8]), ' ')}‚Ç¨")

logger.info("\nAnalyses termin√©es.")

logger.info("Pipeline ETL termin√©.")

# =============================================================================
# CONFIGURATION INTERNATIONALE
# =============================================================================
# Dictionnaire de traduction pour l'internationalisation des modes de travail
TRANSLATIONS: Dict[str, Dict[str, str]] = {
    'fr': {
        'remote': 'T√©l√©travail',
        'in_person': 'Pr√©sentiel',
        'hybrid': 'Hybride'
    }
}

